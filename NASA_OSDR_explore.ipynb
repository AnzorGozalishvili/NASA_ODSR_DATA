{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzdss5Zm17nlaruH6X76Wf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnzorGozalishvili/NASA_ODSR_DATA/blob/main/NASA_OSDR_explore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4apFKHVLZV-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jryOzZcMZBCz"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "YW96GTnyZksY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('anz2/NASA_OSDR')"
      ],
      "metadata": {
        "id": "QoKwqOT3Zmmb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDIXOSGbh5zU",
        "outputId": "95d2633a-e06f-4bf1-bd8b-4587841d1afe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sample Name': 'GSM2684068',\n",
              " 'Protocol REF': 'Nucleic Acid Extraction',\n",
              " 'Parameter Value: DNA Fragmentation': 'sonication',\n",
              " 'Parameter Value: DNA Fragment Size': '200-300 base pair',\n",
              " 'Extract Name': 'GSM2684068',\n",
              " 'Protocol REF.1': 'Library Construction',\n",
              " 'Parameter Value: Library Strategy': 'BisPCR2',\n",
              " 'Parameter Value: Library Selection': 'other',\n",
              " 'Parameter Value: Library Layout': 'PAIRED',\n",
              " 'Protocol REF.2': 'Nucleic Acid Sequencing',\n",
              " 'Parameter Value: Sequencing Instrument': 'Illumina MiSeq',\n",
              " 'Assay Name': 'BisPCR2',\n",
              " 'Parameter Value: Read Length': '150 base pair',\n",
              " 'Raw Data File': 'GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz, GLDS-524_wgbs_GSM2684068_R2_raw.fastq.gz',\n",
              " 'Protocol REF.3': 'GeneLab raw data processing protocol',\n",
              " 'Parameter Value: Read Depth': '199027 read',\n",
              " 'Parameter Value: MultiQC File Names': 'GLDS-524_Gwgbs_raw_multiqc_report.zip'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_file_names = dataset['train'][10]['Raw Data File']"
      ],
      "metadata": {
        "id": "cGQah8oTiaR5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_file_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z-CEcC7sifSB",
        "outputId": "9f219242-d6ec-409b-838d-b52e87ede33e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz, GLDS-524_wgbs_GSM2684068_R2_raw.fastq.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "-ohBhuA5h3aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import os\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config"
      ],
      "metadata": {
        "id": "LY4cmrPijJL0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
      ],
      "metadata": {
        "id": "qkf8FZLUjhvt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NASA_OSDR_BUCKET = 'nasa-osdr'\n",
        "DOWNLOAD_DIR = 'nasa_osdr_fasta_files'\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "5ecadC4PkwkW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_fasta_file_from_s3(fasta_filename):\n",
        "    response = s3.list_objects_v2(Bucket=NASA_OSDR_BUCKET, Prefix='OSD-524')\n",
        "\n",
        "    if 'Contents' in response:\n",
        "        for obj in response['Contents']:\n",
        "            key = obj['Key']\n",
        "            if key.endswith(fasta_filename):\n",
        "                file_path = os.path.join(DOWNLOAD_DIR, fasta_filename)\n",
        "                s3.download_file(Bucket=NASA_OSDR_BUCKET, Key=key, Filename=file_path)\n",
        "                print('file downloaded to path:', file_path)"
      ],
      "metadata": {
        "id": "vyblOLdGkjOU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_file_names.split(',')[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LaSXR2elmAyA",
        "outputId": "797c1f39-09ae-4bd5-8d9c-e28c594cd897"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_fasta_file_from_s3(fasta_file_names.split(',')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awVWhVYWjnLA",
        "outputId": "bbf0e5a0-e40a-4eb2-cba5-1a7b88c72e49"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file downloaded to path: nasa_osdr_fasta_files/GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz\n",
            "file downloaded to path: nasa_osdr_fasta_files/GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls nasa_osdr_fasta_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtG6X5z5mqzm",
        "outputId": "5e3f6a70-628f-45ba-fe5e-300baba48be6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHSynQHCnsYd",
        "outputId": "22ce6325-26f4-4249-8e51-996213e87299"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import SeqIO\n",
        "import gzip"
      ],
      "metadata": {
        "id": "JZNoHBktojIu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_rna_sequences_from_fasta(fasta_file):\n",
        "    with gzip.open(fasta_file, \"rt\") as handle:\n",
        "        for record in SeqIO.parse(handle, \"fastq\"):\n",
        "            yield record"
      ],
      "metadata": {
        "id": "SLKB5cMNoTgI"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_loader = load_rna_sequences_from_fasta('nasa_osdr_fasta_files/GLDS-524_wgbs_GSM2684068_R1_raw.fastq.gz')\n",
        "sample = next(iter(fasta_loader))"
      ],
      "metadata": {
        "id": "dnHKfVfanwcl"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8nG8wYnpxdC",
        "outputId": "9cbbe478-b28b-4f6b-f2ed-39a016f4aa8f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_seq': Seq('GTTTTTTTTAAAGTGTTAGATAGTAGTTAAAATAAGAGATATGTTTTTTATTTA...AGG'),\n",
              " 'id': 'SRR5756273.1',\n",
              " 'name': 'SRR5756273.1',\n",
              " 'description': 'SRR5756273.1 1 length=151',\n",
              " 'dbxrefs': [],\n",
              " 'annotations': {},\n",
              " '_per_letter_annotations': {'phred_quality': [32,\n",
              "   32,\n",
              "   32,\n",
              "   32,\n",
              "   32,\n",
              "   32,\n",
              "   35,\n",
              "   35,\n",
              "   35,\n",
              "   31,\n",
              "   16,\n",
              "   16,\n",
              "   35,\n",
              "   38,\n",
              "   35,\n",
              "   38,\n",
              "   38,\n",
              "   37,\n",
              "   38,\n",
              "   37,\n",
              "   38,\n",
              "   34,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   38,\n",
              "   37,\n",
              "   38,\n",
              "   37,\n",
              "   37,\n",
              "   37,\n",
              "   33,\n",
              "   38,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   38,\n",
              "   38,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   38,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   36,\n",
              "   35,\n",
              "   37,\n",
              "   38,\n",
              "   38,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   38,\n",
              "   38,\n",
              "   38,\n",
              "   38,\n",
              "   38,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   37,\n",
              "   32,\n",
              "   38,\n",
              "   34,\n",
              "   30,\n",
              "   36,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   39,\n",
              "   37,\n",
              "   16]},\n",
              " 'features': []}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mana438/RNABERT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wparXaEtv2Qt",
        "outputId": "e08b7863-f5a1-42e7-dc3f-f9491b1c28da"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RNABERT'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 80 (delta 31), reused 66 (delta 17), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (80/80), 347.43 KiB | 3.66 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install -y g++\n",
        "# !apt-get install build-essential\n",
        "# !cd RNABERT && python setup.py install"
      ],
      "metadata": {
        "id": "c6piQrP_wPzN"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuiQ-1Mn0DZl",
        "outputId": "ff262dec-6653-4a18-b848-abed31e22a45"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nasa_osdr_fasta_files  RNABERT\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RNABERT/module.py\n",
        "import torch\n",
        "# import alignment_C as Aln_C\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "from dataload import  num_to_base\n",
        "import time\n",
        "import numpy as np\n",
        "class Train_Module:\n",
        "\tdef __init__(self, config):\n",
        "\t\tself.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\t\tself.config = config\n",
        "\n",
        "\tdef train_MLM(self, low_seq_0, masked_seq_0, prediction_scores):\n",
        "\t\tcriterion = nn.CrossEntropyLoss()\n",
        "\t\tmask = masked_seq_0 - low_seq_0 != 0\n",
        "\t\tlength_mask = masked_seq_0 != 0\n",
        "\t\tsame_base_mask = torch.bernoulli(torch.ones(mask.shape)*0.05).byte().to(self.device)\n",
        "\t\tmask = mask + same_base_mask\n",
        "\t\tindex = torch.nonzero(mask).split(1, dim=1)\n",
        "\t\tprediction_scores = torch.squeeze(prediction_scores[index])\n",
        "\t\tnew_low_seq_0 = torch.squeeze(low_seq_0[index])\n",
        "\t\tloss = criterion(prediction_scores, new_low_seq_0)\n",
        "\t\t_, preds = torch.max(prediction_scores, 1)\n",
        "\n",
        "\t\tcorrect = torch.sum(preds == new_low_seq_0.data).double()/len(new_low_seq_0)\n",
        "\t\treturn loss, correct\n",
        "\n",
        "\tdef train_SSL(self, SS, prediction_scores):\n",
        "\t\tcriterion = nn.CrossEntropyLoss()\n",
        "\t\tindex = torch.nonzero(SS).split(1, dim=1)\n",
        "\t\tprediction_scores = torch.squeeze(prediction_scores[index])\n",
        "\t\tSS_answer = torch.squeeze(SS[index])\n",
        "\n",
        "\t\tloss = criterion(prediction_scores, SS_answer)\n",
        "\t\t_, preds = torch.max(prediction_scores, 1)\n",
        "\t\tcorrect = torch.sum(preds == SS_answer.data).double()/len(SS_answer)\n",
        "\t\treturn loss, correct\n",
        "\n",
        "\tdef train_SFP(self, low_seq_0, seq_len_0, low_seq_1, seq_len_1, family_0, family_1, z0_list, z1_list):\n",
        "\t\tdistance_label = torch.where((family_0 - family_1) == 0,  torch.ones(family_0.size()[0]), torch.full((family_0.size()[0],), -1) ).to(\"cuda\")\n",
        "\t\tdistance_label_same = torch.where((family_0 - family_1) == 0,  torch.ones(family_0.size()[0]), torch.zeros(family_0.size()[0]) ).to(\"cuda\")\n",
        "\t\tdistance_label_diff = torch.where((family_0 - family_1) != 0,  torch.ones(family_0.size()[0]), torch.zeros(family_0.size()[0]) ).to(\"cuda\")\n",
        "\t\t_, logits = self.match(z0_list, z1_list)\n",
        "\t\tdistance = -logits\n",
        "\t\tdistance_same = torch.dot(distance_label_same , distance.view(-1)) / torch.sum(distance_label_same)\n",
        "\t\tdistance_diff = torch.dot(distance_label_diff , distance.view(-1)) / torch.sum(distance_label_diff)\n",
        "\t\tmargin = 600\n",
        "\t\tloss = F.relu( distance_same - distance_diff + margin )\n",
        "\n",
        "\t\t# accuracy\n",
        "\t\tpredicted_label = torch.where(distance.view(-1) < 660,  torch.ones(family_0.size()[0]).to(\"cuda\"), torch.full((family_0.size()[0],), -1).to(\"cuda\"))\n",
        "\t\tmatch = torch.where((distance_label * predicted_label) == 1, torch.ones(family_0.size()[0]).to(\"cuda\"), torch.zeros(family_0.size()[0]).to(\"cuda\"))\n",
        "\t\tcorrect = torch.sum(match)\n",
        "\t\treturn loss, correct\n",
        "\n",
        "\tdef train_MUL(self, z0_list, z1_list, common_index_0, common_index_1, seq_len_0, seq_len_1):\n",
        "\t\tbert_scores, _ = self.match(z0_list, z1_list)\n",
        "\t\tloss = 0.0\n",
        "\t\tfor i, bert_score in enumerate(bert_scores):\n",
        "\t\t\tloss += self.structural_learning(bert_score, common_index_0[i], seq_len_0[i], common_index_1[i], seq_len_1[i])\n",
        "\t\treturn loss\n",
        "\n",
        "\tdef test_align(self, low_seq_0, low_seq_1, z0_list, z1_list, common_index_0, common_index_1, seq_len_0, seq_len_1, show_aln):\n",
        "\t\tbert_scores, _ = self.match(z0_list, z1_list)\n",
        "\t\tsequence_a = num_to_base(low_seq_0)\n",
        "\t\tsequence_b = num_to_base(low_seq_1)\n",
        "\t\tlen_pred_match = 0\n",
        "\t\tlen_ref_match = 0\n",
        "\t\tlen_TP = 0\n",
        "\t\tfor i, bert_score in enumerate(bert_scores):\n",
        "\t\t\tbert_score = torch.flatten(bert_score.T).tolist()\n",
        "\t\t\tx = 1 if show_aln == True else 0\n",
        "\t\t\tcommon_index_A_B = Aln_C.global_aln(bert_score, [0] * len(bert_score), [0] * len(bert_score), sequence_a[i], sequence_b[i], seq_len_0[i], seq_len_1[i], self.config.gap_opening, self.config.gap_extension, x, 0)\n",
        "\t\t\tcommon_index_A_B = torch.tensor(common_index_A_B).to(self.device).view(2, -1)\n",
        "\t\t\tlen_pred_match += int(torch.sum(common_index_A_B[0]))\n",
        "\t\t\tlen_ref_match += int(torch.sum(common_index_0[i]))\n",
        "\t\t\ta = torch.flatten( (common_index_A_B[0] == 1).nonzero()*10000 + (common_index_A_B[1] == 1).nonzero()).tolist()\n",
        "\t\t\tb = torch.flatten( (common_index_0[i] == 1).nonzero()*10000 + (common_index_1[i] == 1).nonzero()).tolist()\n",
        "\t\t\tlen_TP += len(set(a) & set(b))\n",
        "\t\treturn len_TP, len_pred_match, len_ref_match\n",
        "\n",
        "\tdef structural_learning(self, bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1):\n",
        "\t\treference_alignment_score = self.calc_reference_alignment_score(bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1)\n",
        "\t\tprediction_alignment_score, _ = self.calc_prediction_alignment_score(bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1)\n",
        "\t\treturn prediction_alignment_score - reference_alignment_score\n",
        "\n",
        "\tdef match_bert_score(self, bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1):\n",
        "\t\tindex_0 = (common_index_0 == 1).nonzero()\n",
        "\t\tindex_1 = (common_index_1 == 1).nonzero()\n",
        "\t\tindex = torch.cat([index_0, index_1], axis=1).T\n",
        "\t\tindex = tuple(index)\n",
        "\t\tomega = bert_score[index]\n",
        "\t\treturn omega\n",
        "\n",
        "\tdef margin_matrix(self, bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1):\n",
        "\t\tindex_0 = (common_index_0 == 1).nonzero()\n",
        "\t\tindex_1 = (common_index_1 == 1).nonzero()\n",
        "\t\tindex = torch.cat([index_0, index_1], axis=1).T\n",
        "\t\tindex = tuple(index)\n",
        "\t\tmargin_mat_FP = torch.ones(bert_score.shape).to(self.device) * self.config.margin_FP\n",
        "\t\tmargin_mat_FP[index] = 0.0\n",
        "\t\tmargin_mat_FN = torch.zeros(bert_score.shape).to(self.device)\n",
        "\t\tmargin_mat_FN[index] = self.config.margin_FN\n",
        "\t\treturn margin_mat_FP, margin_mat_FN\n",
        "\n",
        "\tdef margin_score(self, common_index_A_B, common_index_0, common_index_1):\n",
        "\t\tlen_pred_match = int(torch.sum(common_index_A_B[0]))\n",
        "\t\tlen_ref_match = int(torch.sum(common_index_0))\n",
        "\t\ta = torch.flatten( (common_index_A_B[0] == 1).nonzero()*10000 + (common_index_A_B[1] == 1).nonzero()).tolist()\n",
        "\t\tb = torch.flatten( (common_index_0 == 1).nonzero()*10000 + (common_index_1 == 1).nonzero()).tolist()\n",
        "\t\tlen_TP = len(set(a) & set(b))\n",
        "\t\tlen_FP = len_pred_match - len_TP\n",
        "\t\tlen_FN = len_ref_match - len_TP\n",
        "\t\treturn len_FP * self.config.margin_FP + len_FN * self.config.margin_FN\n",
        "\n",
        "\tdef calc_reference_alignment_score(self, bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1):\n",
        "\t\treference_alignment_score = 0\n",
        "\t\treference_alignment_score += self.gapscore(common_index_0, seq_len_0)\n",
        "\t\treference_alignment_score += self.gapscore(common_index_1, seq_len_1)\n",
        "\t\tomega = self.match_bert_score(bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1)\n",
        "\t\treference_alignment_score += torch.sum(omega)\n",
        "\t\treturn reference_alignment_score\n",
        "\n",
        "\tdef calc_prediction_alignment_score(self, bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1):\n",
        "\t\tsequence_a = \"N\"*seq_len_0\n",
        "\t\tsequence_b = \"N\"*seq_len_1\n",
        "\t\tmargin_mat_FP, margin_mat_FN = self.margin_matrix(bert_score, common_index_0, seq_len_0, common_index_1, seq_len_1)\n",
        "\t\tcommon_index_A_B = Aln_C.global_aln(torch.flatten(bert_score.T).tolist(), torch.flatten(margin_mat_FP.T).tolist(), torch.flatten(margin_mat_FN.T).tolist(), sequence_a, sequence_b, seq_len_0, seq_len_1, self.config.gap_opening, self.config.gap_extension, 0, 0)\n",
        "\t\tcommon_index_A_B = torch.tensor(common_index_A_B).to(self.device).view(2, -1)\n",
        "\t\tprediction_alignment_score = self.calc_reference_alignment_score(bert_score, common_index_A_B[0], seq_len_0, common_index_A_B[1], seq_len_1) + self.margin_score(common_index_A_B, common_index_0, common_index_1)\n",
        "\t\treturn prediction_alignment_score, common_index_A_B\n",
        "\n",
        "\tdef gapscore(self, index, seq_len):\n",
        "\t\tseq_len = int(seq_len)\n",
        "\t\tindex = index[:seq_len].to('cpu').detach().numpy().copy()\n",
        "\t\tall_zeros = seq_len - np.count_nonzero(index)\n",
        "\t\textend_zeros = seq_len - np.count_nonzero(np.insert(index[:-1], 0, 1) + index)\n",
        "\t\topen_zeros = all_zeros - extend_zeros\n",
        "\t\treturn (-1*self.config.gap_opening + -1*self.config.gap_extension ) * open_zeros + -1*self.config.gap_extension * extend_zeros\n",
        "\n",
        "\tdef match(self,z0_list, z1_list):\n",
        "\t\tmatch_scores = []\n",
        "\t\tlogits = []\n",
        "\t\tfor z0, z1 in zip(z0_list, z1_list):\n",
        "\t\t\t# cos similarity\n",
        "\t\t\tmatch_score = nn.CosineSimilarity(dim=2, eps=1e-6)(z0.unsqueeze(1).repeat(1, z1.shape[0],1) , z1.unsqueeze(0).repeat(z0.shape[0], 1,1))\n",
        "\t\t\ts = 1.3 * match_score\n",
        "\t\t\t# L1 distance\n",
        "\t\t\t# s = -torch.sum(torch.abs(z0.unsqueeze(1)-z1), -1)\n",
        "\t\t\t# match_score = torch.exp(s)\n",
        "\n",
        "\t\t\t# soft align\n",
        "\t\t\ta, b = F.softmax(s, 1), F.softmax(s, 0)\n",
        "\t\t\tc = a + b - a*b\n",
        "\t\t\tc = torch.sum(c*s)/torch.sum(c)\n",
        "\t\t\tmatch_scores.append(match_score)\n",
        "\t\t\tlogits.append(c.view(-1))\n",
        "\t\tlogits = torch.stack(logits, 0)\n",
        "\t\treturn match_scores, logits\n",
        "\n",
        "\tdef em(self, h, lengths):\n",
        "\t\t# get representations with different lengths from the collated single matrix\n",
        "\t\te = [None] * len(lengths)\n",
        "\t\tfor i in range(len(lengths)):\n",
        "\t\t\te[i] = h[i, :lengths[i]]\n",
        "\t\treturn e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMqC0L82w9re",
        "outputId": "651dc3a0-54d5-4153-fd46-92f9e948c2ee"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting RNABERT/module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RNABERT/MLM_SFP.py\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "import copy\n",
        "from Bio import SeqIO\n",
        "import argparse\n",
        "from utils.bert import get_config, BertModel, set_learned_params, BertForMaskedLM, visualize_attention, show_base_PCA, fix_params\n",
        "from module import Train_Module\n",
        "from dataload import DATA, MyDataset\n",
        "import datetime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_score\n",
        "import torch.nn.functional as F\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans, AgglomerativeClustering, SpectralClustering\n",
        "import itertools\n",
        "\n",
        "# import alignment_C as Aln_C\n",
        "\n",
        "random.seed(10)\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='RNABERT')\n",
        "parser.add_argument('--mag',  type=int, default=1,\n",
        "                    help='enumerate')\n",
        "parser.add_argument('--epoch', '-e', type=int, default=200,\n",
        "                    help='Number of sweeps over the dataset to train')\n",
        "parser.add_argument('--batch', '-b', type=int, default=20,\n",
        "                    help='Number of batch size')\n",
        "parser.add_argument('--maskrate', '-m', type=float, default=0.0,\n",
        "                    help='mask rate')\n",
        "parser.add_argument('--pretraining', '-pre', type=str, help='use pretrained weight')\n",
        "parser.add_argument('--outputweight', type=str, help='output path for weights')\n",
        "parser.add_argument('--algorithm', type=str, default=\"global\", help='algorithm method')\n",
        "parser.add_argument('--data_mlm', '-d', type=str, nargs='*', help='data for mlm training')\n",
        "parser.add_argument('--data_mul', type=str, nargs='*', help='data for mul training')\n",
        "parser.add_argument('--data_alignment', type=str, nargs='*', help='data for alignment test')\n",
        "parser.add_argument('--data_clustering', type=str, nargs='*', help='data for clustering test')\n",
        "parser.add_argument('--data_showbase', type=str, nargs='*', help='data for base embedding')\n",
        "parser.add_argument('--data_embedding', type=str, nargs='*', help='data for base embedding')\n",
        "parser.add_argument('--embedding_output', type=str, nargs='*', help='output file for base embedding')\n",
        "parser.add_argument('--show_aln', action='store_true')\n",
        "\n",
        "args = parser.parse_args()\n",
        "batch_size = args.batch\n",
        "current_time = datetime.datetime.now()\n",
        "\n",
        "print(\"start...\")\n",
        "class TRAIN:\n",
        "    \"\"\"The class for controlling the training process of SFP\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.module = Train_Module(config)\n",
        "\n",
        "    def model_device(self, model):\n",
        "        print(\"device: \", self.device)\n",
        "        print('-----start-------')\n",
        "        model.to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            model = torch.nn.DataParallel(model) # make parallel\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        return model\n",
        "\n",
        "    def train_MLM_SFP(self, model, optimizer, dl_MLM_SFP, num_epochs, task_type):\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            epoch_mlm_loss = 0.0\n",
        "            epoch_ssl_loss = 0.0\n",
        "            epoch_mlm_correct = 0.0\n",
        "            epoch_ssl_correct = 0.0\n",
        "            epoch_sfp_loss=0.0\n",
        "            epoch_sfp_correct = 0.0\n",
        "            epoch_mul_loss = 0.0\n",
        "\n",
        "            iteration = 1\n",
        "            t_epoch_start = time.time()\n",
        "            t_iter_start = time.time()\n",
        "            data_num = 0\n",
        "            for batch in dl_MLM_SFP:\n",
        "                optimizer.zero_grad()\n",
        "                if task_type == \"MLM\" or task_type == \"SFP\":\n",
        "                    low_seq_0, masked_seq_0, family_0, seq_len_0, low_seq_1, masked_seq_1, family_1, seq_len_1 = batch\n",
        "                elif task_type == \"MUL\":\n",
        "                    low_seq_0, masked_seq_0, family_0, seq_len_0, low_seq_1, masked_seq_1, family_1, seq_len_1, common_index_0, common_index_1 = batch\n",
        "\n",
        "                masked_seq_0 = masked_seq_0.to(self.device)\n",
        "                low_seq_0 = low_seq_0.to(self.device)\n",
        "                masked_seq_1 = masked_seq_1.to(self.device)\n",
        "                low_seq_1 = low_seq_1.to(self.device)\n",
        "\n",
        "                masked_seq = torch.cat((masked_seq_0, masked_seq_1), axis=0)\n",
        "                prediction_scores, prediction_scores_ss, encoded_layers =  model(masked_seq)\n",
        "                prediction_scores0, prediction_scores1 = torch.split(prediction_scores, int(prediction_scores.shape[0]/2))\n",
        "                prediction_scores_ss0, prediction_scores_ss1 = torch.split(prediction_scores_ss, int(prediction_scores_ss.shape[0]/2))\n",
        "                encoded_layers0, encoded_layers1 = torch.split(encoded_layers, int(encoded_layers.shape[0]/2))\n",
        "\n",
        "                loss = 0\n",
        "                # MLM LOSS\n",
        "                mlm_loss_0, mlm_correct_0 = self.module.train_MLM(low_seq_0, masked_seq_0, prediction_scores0)\n",
        "                mlm_loss_1, mlm_correct_1 = self.module.train_MLM(low_seq_1, masked_seq_1, prediction_scores1)\n",
        "                mlm_loss = (mlm_loss_0 + mlm_loss_1)/2\n",
        "                mlm_loss = torch.tensor(0.0) if  torch.isnan(mlm_loss) else mlm_loss\n",
        "                mlm_correct = (mlm_correct_0 + mlm_correct_1)/2\n",
        "                epoch_mlm_loss += mlm_loss.item() * batch_size\n",
        "                epoch_mlm_correct += mlm_correct\n",
        "                if task_type == \"MLM\":\n",
        "                    loss += mlm_loss\n",
        "\n",
        "                # SFP LOSS\n",
        "                if task_type == \"SFP\":\n",
        "                    z0_list, z1_list =  self.module.em(encoded_layers0, seq_len_0), self.module.em(encoded_layers1, seq_len_1)\n",
        "                    sfp_loss, sfp_correct = self.module.train_SFP(low_seq_0, seq_len_0, low_seq_1, seq_len_1, family_0, family_1, z0_list, z1_list)\n",
        "                    sfp_loss = torch.tensor(0.0) if  torch.isnan(sfp_loss) else sfp_loss\n",
        "                    epoch_sfp_loss += sfp_loss.item()* batch_size\n",
        "                    epoch_sfp_correct += sfp_correct\n",
        "                    loss += sfp_loss\n",
        "\n",
        "                # MULTIPLE LOSS\n",
        "                if task_type == \"MUL\":\n",
        "                    common_index_0 = common_index_0.to(self.device)\n",
        "                    common_index_1 = common_index_1.to(self.device)\n",
        "                    z0_list, z1_list =  self.module.em(encoded_layers0, seq_len_0), self.module.em(encoded_layers1, seq_len_1)\n",
        "                    mul_loss = self.module.train_MUL(z0_list, z1_list, common_index_0, common_index_1, seq_len_0, seq_len_1)\n",
        "                    mul_loss = torch.tensor(0.0) if  torch.isnan(mul_loss) else mul_loss\n",
        "                    epoch_mul_loss += mul_loss.item()\n",
        "                    loss +=  mul_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            t_epoch_finish = time.time()\n",
        "            epoch_mlm_loss = epoch_mlm_loss / len(dl_MLM_SFP.dataset)\n",
        "            epoch_mlm_correct = epoch_mlm_correct / len(dl_MLM_SFP)\n",
        "            epoch_sfp_loss = epoch_sfp_loss  / len(dl_MLM_SFP.dataset)\n",
        "            epoch_sfp_correct = epoch_sfp_correct / len(dl_MLM_SFP.dataset)\n",
        "            epoch_mul_loss = epoch_mul_loss\n",
        "            print('Epoch {}/{} | MLM Loss: {:.4f} MLM Acc: {:.4f}| SFP Loss: {:.4f} SFP Acc: {:.4f}| MUL Loss: {:.4f}| time: {:.4f} sec.'.format(epoch+1, num_epochs,\n",
        "                                                                        epoch_mlm_loss, epoch_mlm_correct, epoch_sfp_loss, epoch_sfp_correct, epoch_mul_loss, time.time() - t_epoch_start))\n",
        "            t_epoch_start = time.time()\n",
        "        if args.outputweight:\n",
        "            torch.save(model.state_dict(), args.outputweight + '{0:%m_%d_%H_%M}'.format(current_time))\n",
        "            torch.save(model.state_dict(), args.outputweight)\n",
        "        return model\n",
        "\n",
        "    # make feature vector\n",
        "    def make_feature(self, model, dataloader, seqs):\n",
        "        model.eval()\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        batch_size = dataloader.batch_size\n",
        "        encoding = []\n",
        "        for batch in dataloader:\n",
        "            data, label, seq_len= batch\n",
        "            inputs = data.to(self.device)\n",
        "            prediction_scores, prediction_scores_ss, encoded_layers =  model(inputs)\n",
        "            encoding.append(encoded_layers.cpu().detach().numpy())\n",
        "        encoding = np.concatenate(encoding, 0)\n",
        "\n",
        "        embedding = []\n",
        "        for e, seq in zip(encoding, seqs):\n",
        "            embedding.append(e[:len(seq)].tolist())\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def validateOnCompleteTestData(self, test_loader, simirality_matrix):\n",
        "        # accuracy and rand index\n",
        "        nmi = normalized_mutual_info_score\n",
        "        ari = adjusted_rand_score\n",
        "        homo = homogeneity_score\n",
        "        com = completeness_score\n",
        "        true_labels = np.concatenate([d[1].cpu().numpy() for i,d in enumerate(test_loader)], 0)\n",
        "\n",
        "        # km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20, n_jobs=4)\n",
        "        # y_pred = km.fit_predict(simirality_matrix)\n",
        "\n",
        "        # ac = AgglomerativeClustering(n_clusters=len(np.unique(true_labels)), affinity='precomputed', linkage='average')\n",
        "        # ac = AgglomerativeClustering(n_clusters=None,affinity='precomputed', linkage='average', distance_threshold=0.45)\n",
        "        # y_pred = ac.fit_predict(1+ (-1 * simirality_matrix))\n",
        "\n",
        "        # y_pred = y_pred.tolist()\n",
        "        # true_labels = true_labels.tolist()\n",
        "        # import collections\n",
        "        # c = collections.Counter(y_pred)\n",
        "        # y_pred_new = []\n",
        "        # true_labels_new = []\n",
        "        # for i, j in zip(y_pred, true_labels):\n",
        "        #     if c[i] >= 2:\n",
        "        #         y_pred_new.append(i)\n",
        "        #         true_labels_new.append(j)\n",
        "        # print(len(y_pred_new))\n",
        "        # y_pred = np.array(y_pred_new)\n",
        "        # true_labels = np.array(true_labels_new)\n",
        "\n",
        "        sc=SpectralClustering(n_clusters=len(np.unique(true_labels)))\n",
        "        y_pred=sc.fit(simirality_matrix).labels_\n",
        "\n",
        "        print(' '*8 + '|==>  nmi: %.4f ,  ari: %.4f,  com: %.4f,  homo: %.4f  <==|'\n",
        "                      % (nmi(true_labels, y_pred), ari(true_labels, y_pred), com(true_labels, y_pred), homo(true_labels, y_pred)))\n",
        "        return ari(true_labels, y_pred)\n",
        "\n",
        "    def align(self, model, dl):\n",
        "        model.eval()\n",
        "        pred_match = 0\n",
        "        ref_match = 0\n",
        "        TP = 0\n",
        "        for batch in dl:\n",
        "            low_seq_0, masked_seq_0, family_0, seq_len_0, low_seq_1, masked_seq_1, family_1, seq_len_1, common_index_0, common_index_1 = batch\n",
        "            low_seq_0 = low_seq_0.to(self.device)\n",
        "            low_seq_1 = low_seq_1.to(self.device)\n",
        "            low_seq = torch.cat((low_seq_0, low_seq_1), axis=0)\n",
        "\n",
        "            start = time.time()\n",
        "            prediction_scores, prediction_scores_ss, encoded_layers =  model(low_seq)\n",
        "            elapsed_time = time.time() - start\n",
        "            # print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "            prediction_scores0, prediction_scores1 = torch.split(prediction_scores, int(prediction_scores.shape[0]/2))\n",
        "            encoded_layers0, encoded_layers1 = torch.split(encoded_layers, int(encoded_layers.shape[0]/2))\n",
        "            z0_list, z1_list =  self.module.em(encoded_layers0, seq_len_0), self.module.em(encoded_layers1, seq_len_1)\n",
        "            len_TP, len_pred_match, len_ref_match = self.module.test_align(low_seq_0, low_seq_1, z0_list, z1_list, common_index_0, common_index_1, seq_len_0, seq_len_1, args.show_aln)\n",
        "            TP += len_TP\n",
        "            pred_match += len_pred_match\n",
        "            ref_match += len_ref_match\n",
        "\n",
        "        PPV = TP /pred_match\n",
        "        sens = TP / ref_match\n",
        "        f1 = 2 * PPV * sens/(PPV + sens)\n",
        "        if args.show_aln == False:\n",
        "            print(\"alignment accuracy : \", f1, \"sens : \", sens, \"PPV : \", PPV)\n",
        "        return f1\n",
        "\n",
        "    def test(self, ds, test_loader, model):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.eval()\n",
        "        data_num = len(test_loader.dataset)\n",
        "        simirality_matrix = []\n",
        "        for i in range(data_num):\n",
        "            single_seq = MyDataset(\"CLU\", np.tile(ds.low_seq[i],(data_num,1)), np.tile(ds.low_seq[i],(data_num,1)),np.tile(ds.family[i],(data_num,1)), np.tile(ds.seq_len[i], data_num))\n",
        "            single_seq = torch.utils.data.DataLoader(single_seq, batch_size, shuffle=False)\n",
        "            low = []\n",
        "            for data0, data1 in zip( test_loader, single_seq):\n",
        "                x0, label0, seq_len_0 = data0\n",
        "                x1, label1, seq_len_1 = data1\n",
        "                x0, label0 = x0.to(device),label0.to(device),\n",
        "                x1, label1 = x1.to(device),label1.to(device),\n",
        "                x = torch.cat((x0, x1), axis=0)\n",
        "                prediction_scores, prediction_scores_ss, encoded_layers =  model(x)\n",
        "                encoded_layers0, encoded_layers1 = torch.split(encoded_layers, int(encoded_layers.shape[0]/2))\n",
        "                z0_list, z1_list =  self.module.em(encoded_layers0, seq_len_0), self.module.em(encoded_layers1, seq_len_1)\n",
        "                _, logits = self.module.match(z0_list, z1_list)\n",
        "                low.append(torch.squeeze(logits).to('cpu').detach().numpy().copy())\n",
        "            simirality_matrix.append(np.concatenate(low, 0))\n",
        "        currentAcc = self.validateOnCompleteTestData(test_loader, np.array(simirality_matrix))\n",
        "        return currentAcc\n",
        "\n",
        "def objective():\n",
        "    config.hidden_size = config.num_attention_heads * config.multiple\n",
        "    train = TRAIN(config)\n",
        "    model = BertModel(config)\n",
        "    model = BertForMaskedLM(config, model)\n",
        "    if args.data_mlm:\n",
        "        config.adam_lr = 2e-4\n",
        "    # if args.data_sfp:\n",
        "    #     model = fix_params(model)\n",
        "    #     config.adam_lr = config.adam_lr * 0.5\n",
        "    if args.data_mul:\n",
        "        # model = fix_params(model)\n",
        "        config.adam_lr = 1e-4\n",
        "    model = train.model_device(model)\n",
        "    if args.pretraining:\n",
        "        state_dict = torch.load(args.pretraining, map_location=torch.device(train.device))\n",
        "        new_state_dict = {key.lstrip('module.'): value for key, value in state_dict.items()}\n",
        "        model.load_state_dict(new_state_dict,  strict=False)\n",
        "    optimizer = optim.AdamW([{'params': model.parameters(), 'lr': config.adam_lr}])\n",
        "    return model , optimizer, train, config\n",
        "\n",
        "config = get_config(file_path = \"./RNA_bert_config.json\")\n",
        "data = DATA(args, config)\n",
        "model, optimizer, train, config = objective()\n",
        "\n",
        "#now start training\n",
        "if args.data_mlm:\n",
        "    dl_MLM = data.load_data_MLM_SFP(args.data_mlm)\n",
        "    model = train.train_MLM_SFP(model, optimizer, dl_MLM, args.epoch, \"MLM\")\n",
        "# elif args.data_sfp:\n",
        "#     dl_SFP = data.load_data_MLM_SFP(args.data_sfp)\n",
        "#     model = train.train_MLM_SFP(model, optimizer, dl_SFP, args.epoch, \"SFP\")\n",
        "if args.data_mul:\n",
        "    dl_MUL = data.load_data_MUL(args.data_mul, \"MUL\")\n",
        "    model = train.train_MLM_SFP(model, optimizer, dl_MUL, args.epoch, \"MUL\")\n",
        "\n",
        "if args.data_alignment:\n",
        "    dl_alignment = data.load_data_MUL(args.data_alignment, \"MUL\")\n",
        "    alignment_accuracy = train.align(model, dl_alignment)\n",
        "elif args.data_clustering:\n",
        "    _, _, ds, test_dl = data.load_data_CLU(args.data_clustering)\n",
        "    train.test(ds, test_dl, model)\n",
        "\n",
        "if args.data_showbase:\n",
        "    seqs, label, SS,  ds, test_dl  = data.load_data_SHOW(args.data_showbase)\n",
        "    features = train.make_feature(model, test_dl)\n",
        "    features = features.reshape(-1, features.shape[2])\n",
        "    show_base_PCA(features, label.reshape(-1), SS)\n",
        "\n",
        "if args.data_embedding:\n",
        "    seqs, label, test_dl  = data.load_data_EMB(args.data_embedding)\n",
        "    features = train.make_feature(model, test_dl, seqs)\n",
        "    for i, data_set in enumerate(args.embedding_output):\n",
        "        with open(data_set, 'w') as f:\n",
        "            for d in features:\n",
        "                f.write(str(d) + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvjz7TA80Lrm",
        "outputId": "15aaecda-f069-4ed1-99ba-05261859d4f7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting RNABERT/MLM_SFP.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install python 3.9 and dev utils\n",
        "#you may not need all the dev libraries, but I haven't tested which aren't necessary.\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.9 python3.9-dev python3.9-distutils libpython3.9-dev\n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n",
        "\n",
        "#Check that it points at the right location\n",
        "!python3 --version\n",
        "\n",
        "# install pip\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3 get-pip.py --force-reinstall\n",
        "\n",
        "#install colab's dependencies\n",
        "!python3 -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "# link to the old google package\n",
        "!ln -s /usr/local/lib/python3.8/dist-packages/google \\\n",
        "       /usr/local/lib/python3.9/dist-packages/google\n",
        "\n",
        "# There has got to be a better way to do this...but there's a bad import in some of the colab files\n",
        "# IPython no longer exposes traitlets like this, it's a separate package now\n",
        "!sed -i \"s/from IPython.utils import traitlets as _traitlets/import traitlets as _traitlets/\" /usr/local/lib/python3.9/dist-packages/google/colab/*.py\n",
        "!sed -i \"s/from IPython.utils import traitlets/import traitlets/\" /usr/local/lib/python3.9/dist-packages/google/colab/*.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvq66tui19KV",
        "outputId": "5119bd3a-fea7-4ac3-fa83-632929c1856d"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 0 B/110 kB \r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 75.0 kB/110 kB 68%] [2 InRelease 0 B/3,62\r0% [Waiting for headers] [1 InRelease 110 kB/110 kB 100%] [Connected to ppa.lau\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,266 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,342 kB]\n",
            "Fetched 2,950 kB in 2s (1,405 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.9 libpython3.9-minimal libpython3.9-stdlib mailcap mime-support\n",
            "  python3.9-lib2to3 python3.9-minimal\n",
            "Suggested packages:\n",
            "  python3.9-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.9 libpython3.9-dev libpython3.9-minimal libpython3.9-stdlib\n",
            "  mailcap mime-support python3.9 python3.9-dev python3.9-distutils\n",
            "  python3.9-lib2to3 python3.9-minimal\n",
            "0 upgraded, 11 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 12.6 MB of archives.\n",
            "After this operation, 47.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-minimal amd64 3.9.18-1+jammy1 [835 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-minimal amd64 3.9.18-1+jammy1 [2,079 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-stdlib amd64 3.9.18-1+jammy1 [1,841 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9 amd64 3.9.18-1+jammy1 [1,904 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-dev amd64 3.9.18-1+jammy1 [4,628 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9 amd64 3.9.18-1+jammy1 [496 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-dev amd64 3.9.18-1+jammy1 [500 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-lib2to3 all 3.9.18-1+jammy1 [127 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-distutils all 3.9.18-1+jammy1 [193 kB]\n",
            "Fetched 12.6 MB in 9s (1,451 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 11.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.9-minimal:amd64.\n",
            "(Reading database ... 120875 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython3.9-minimal_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-minimal:amd64 (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-minimal.\n",
            "Preparing to unpack .../01-python3.9-minimal_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9-minimal (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../02-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../03-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.9-stdlib:amd64.\n",
            "Preparing to unpack .../04-libpython3.9-stdlib_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-stdlib:amd64 (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.9:amd64.\n",
            "Preparing to unpack .../05-libpython3.9_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9:amd64 (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.9-dev:amd64.\n",
            "Preparing to unpack .../06-libpython3.9-dev_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-dev:amd64 (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9.\n",
            "Preparing to unpack .../07-python3.9_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9 (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-dev.\n",
            "Preparing to unpack .../08-python3.9-dev_3.9.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9-dev (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-lib2to3.\n",
            "Preparing to unpack .../09-python3.9-lib2to3_3.9.18-1+jammy1_all.deb ...\n",
            "Unpacking python3.9-lib2to3 (3.9.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-distutils.\n",
            "Preparing to unpack .../10-python3.9-distutils_3.9.18-1+jammy1_all.deb ...\n",
            "Unpacking python3.9-distutils (3.9.18-1+jammy1) ...\n",
            "Setting up libpython3.9-minimal:amd64 (3.9.18-1+jammy1) ...\n",
            "Setting up python3.9-lib2to3 (3.9.18-1+jammy1) ...\n",
            "Setting up python3.9-distutils (3.9.18-1+jammy1) ...\n",
            "Setting up python3.9-minimal (3.9.18-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.9-stdlib:amd64 (3.9.18-1+jammy1) ...\n",
            "Setting up libpython3.9:amd64 (3.9.18-1+jammy1) ...\n",
            "Setting up python3.9 (3.9.18-1+jammy1) ...\n",
            "Setting up libpython3.9-dev:amd64 (3.9.18-1+jammy1) ...\n",
            "Setting up python3.9-dev (3.9.18-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "update-alternatives: error: alternative path /usr/bin/python3.8 doesn't exist\n",
            "update-alternatives: using /usr/bin/python3.9 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Python 3.9.18\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2544k  100 2544k    0     0   9.9M      0 --:--:-- --:--:-- --:--:--  9.9M\n",
            "Collecting pip\n",
            "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/50/c2/e06851e8cc28dcad7c155f4753da8833ac06a5c704c109313b8d5a62968a/pip-23.2.1-py3-none-any.whl.metadata\n",
            "  Downloading pip-23.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting setuptools\n",
            "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/bb/26/7945080113158354380a12ce26873dd6c1ebd88d47f5bc24e2c5bb38c16a/setuptools-68.2.2-py3-none-any.whl.metadata\n",
            "  Downloading setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting wheel\n",
            "  Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/b8/8b/31273bf66016be6ad22bb7345c37ff350276cfd46e389a0c2ac5da9d9073/wheel-0.41.2-py3-none-any.whl.metadata\n",
            "  Downloading wheel-0.41.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.41.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
            "Successfully installed pip-23.2.1 setuptools-68.2.2 wheel-0.41.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ipython\n",
            "  Obtaining dependency information for ipython from https://files.pythonhosted.org/packages/ef/02/fc039fca3ec40a00f962eb6e9da45c507334b0650a3cb9facd38d234fb7a/ipython-8.16.1-py3-none-any.whl.metadata\n",
            "  Downloading ipython-8.16.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting ipython_genutils\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting ipykernel\n",
            "  Obtaining dependency information for ipykernel from https://files.pythonhosted.org/packages/94/e3/70fb6e6bdd42cb0586d6b6680713997d2a9dc46642aec207ca04d0df80b8/ipykernel-6.25.2-py3-none-any.whl.metadata\n",
            "  Downloading ipykernel-6.25.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jupyter_console\n",
            "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
            "Collecting prompt_toolkit\n",
            "  Obtaining dependency information for prompt_toolkit from https://files.pythonhosted.org/packages/a9/b4/ba77c84edf499877317225d7b7bc047a81f7c2eed9628eeb6bab0ac2e6c9/prompt_toolkit-3.0.39-py3-none-any.whl.metadata\n",
            "  Downloading prompt_toolkit-3.0.39-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting httplib2\n",
            "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astor\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting backcall (from ipython)\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting decorator (from ipython)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Obtaining dependency information for jedi>=0.16 from https://files.pythonhosted.org/packages/20/9f/bc63f0f0737ad7a60800bfd472a4836661adae21f9c2535f3957b1e54ceb/jedi-0.19.1-py2.py3-none-any.whl.metadata\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting matplotlib-inline (from ipython)\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting pickleshare (from ipython)\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting pygments>=2.4.0 (from ipython)\n",
            "  Obtaining dependency information for pygments>=2.4.0 from https://files.pythonhosted.org/packages/43/88/29adf0b44ba6ac85045e63734ae0997d3c58d8b1a91c914d240828d0d73d/Pygments-2.16.1-py3-none-any.whl.metadata\n",
            "  Downloading Pygments-2.16.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting stack-data (from ipython)\n",
            "  Obtaining dependency information for stack-data from https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl.metadata\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5 (from ipython)\n",
            "  Obtaining dependency information for traitlets>=5 from https://files.pythonhosted.org/packages/85/e9/d82415708306eb348fb16988c4697076119dfbfa266f17f74e514a23a723/traitlets-5.11.2-py3-none-any.whl.metadata\n",
            "  Downloading traitlets-5.11.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting typing-extensions (from ipython)\n",
            "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting exceptiongroup (from ipython)\n",
            "  Obtaining dependency information for exceptiongroup from https://files.pythonhosted.org/packages/ad/83/b71e58666f156a39fb29417e4c8ca4bc7400c0dd4ed9e8842ab54dc8c344/exceptiongroup-1.1.3-py3-none-any.whl.metadata\n",
            "  Downloading exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pexpect>4.3 (from ipython)\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel)\n",
            "  Obtaining dependency information for comm>=0.1.1 from https://files.pythonhosted.org/packages/fe/47/0133ac1b7dc476ed77710715e98077119b3d9bae56b13f6f9055e7da1c53/comm-0.1.4-py3-none-any.whl.metadata\n",
            "  Downloading comm-0.1.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Obtaining dependency information for debugpy>=1.6.5 from https://files.pythonhosted.org/packages/f7/79/6053a14f60e879b8cb1a42dadbbef8822603eadde79944c61e955309b622/debugpy-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading debugpy-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Obtaining dependency information for jupyter-client>=6.1.12 from https://files.pythonhosted.org/packages/73/d4/3c13d6a300be9e894561aea0b81e7aed46e8f98029b7d9369d90b1fc7ac5/jupyter_client-8.3.1-py3-none-any.whl.metadata\n",
            "  Downloading jupyter_client-8.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Obtaining dependency information for jupyter-core!=5.0.*,>=4.12 from https://files.pythonhosted.org/packages/bf/70/7b8dbda173b97be0ad40c5eb673bb1901cfeac29554d30cf9df49e59a694/jupyter_core-5.3.2-py3-none-any.whl.metadata\n",
            "  Downloading jupyter_core-5.3.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Obtaining dependency information for nest-asyncio from https://files.pythonhosted.org/packages/ab/d3/48c01d1944e0ee49fdc005bf518a68b0582d3bd201e5401664890b62a647/nest_asyncio-1.5.8-py3-none-any.whl.metadata\n",
            "  Downloading nest_asyncio-1.5.8-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting packaging (from ipykernel)\n",
            "  Obtaining dependency information for packaging from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting psutil (from ipykernel)\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzmq>=20 (from ipykernel)\n",
            "  Obtaining dependency information for pyzmq>=20 from https://files.pythonhosted.org/packages/a2/e0/08605421a2ede5d87adbde9685599fa7e6af1df700c657759a1892ced942/pyzmq-25.1.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
            "  Downloading pyzmq-25.1.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Obtaining dependency information for tornado>=6.1 from https://files.pythonhosted.org/packages/66/a5/e6da56c03ff61200d5a43cfb75ab09316fc0836aa7ee26b4e9dcbfc3ae85/tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting wcwidth (from prompt_toolkit)\n",
            "  Obtaining dependency information for wcwidth from https://files.pythonhosted.org/packages/58/19/a9ce39f89cf58cf1e7ce01c8bb76ab7e2c7aadbc5a2136c3e192097344f5/wcwidth-0.2.8-py2.py3-none-any.whl.metadata\n",
            "  Downloading wcwidth-0.2.8-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2)\n",
            "  Obtaining dependency information for pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython)\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata>=4.8.3 (from jupyter-client>=6.1.12->ipykernel)\n",
            "  Obtaining dependency information for importlib-metadata>=4.8.3 from https://files.pythonhosted.org/packages/cc/37/db7ba97e676af155f5fcb1a35466f446eadc9104e25b83366e8088c9c926/importlib_metadata-6.8.0-py3-none-any.whl.metadata\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from jupyter-client>=6.1.12->ipykernel)\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
            "  Obtaining dependency information for platformdirs>=2.5 from https://files.pythonhosted.org/packages/56/29/3ec311dc18804409ecf0d2b09caa976f3ae6215559306b5b530004e11156/platformdirs-3.11.0-py3-none-any.whl.metadata\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython)\n",
            "  Obtaining dependency information for executing>=1.2.0 from https://files.pythonhosted.org/packages/bb/3f/748594706233e45fd0e6fb57a2fbfe572485009c52b19919d161a0ae5d52/executing-2.0.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading executing-2.0.0-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython)\n",
            "  Obtaining dependency information for asttokens>=2.1.0 from https://files.pythonhosted.org/packages/4f/25/adda9979586d9606300415c89ad0e4c5b53d72b92d2747a3c634701a6a02/asttokens-2.4.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading asttokens-2.4.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pure-eval (from stack-data->ipython)\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n",
            "Collecting zipp>=0.5 (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel)\n",
            "  Obtaining dependency information for zipp>=0.5 from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading ipython-8.16.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.0/807.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.25.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.39-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
            "Downloading debugpy-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.3.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.3.2-py3-none-any.whl (28 kB)\n",
            "Downloading Pygments-2.16.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-25.1.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.11.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Downloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Downloading wcwidth-0.2.8-py2.py3-none-any.whl (31 kB)\n",
            "Downloading asttokens-2.4.0-py2.py3-none-any.whl (27 kB)\n",
            "Downloading executing-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, pickleshare, ipython_genutils, executing, backcall, zipp, typing-extensions, traitlets, tornado, pyzmq, python-dateutil, pyparsing, pygments, psutil, prompt_toolkit, platformdirs, pexpect, parso, packaging, nest-asyncio, exceptiongroup, decorator, debugpy, asttokens, astor, stack-data, matplotlib-inline, jupyter-core, jedi, importlib-metadata, httplib2, comm, jupyter-client, ipython, ipykernel, jupyter_console\n",
            "Successfully installed astor-0.8.1 asttokens-2.4.0 backcall-0.2.0 comm-0.1.4 debugpy-1.8.0 decorator-5.1.1 exceptiongroup-1.1.3 executing-2.0.0 httplib2-0.22.0 importlib-metadata-6.8.0 ipykernel-6.25.2 ipython-8.16.1 ipython_genutils-0.2.0 jedi-0.19.1 jupyter-client-8.3.1 jupyter-core-5.3.2 jupyter_console-6.6.3 matplotlib-inline-0.1.6 nest-asyncio-1.5.8 packaging-23.2 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-3.11.0 prompt_toolkit-3.0.39 psutil-5.9.5 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.16.1 pyparsing-3.1.1 python-dateutil-2.8.2 pyzmq-25.1.1 stack-data-0.6.3 tornado-6.3.3 traitlets-5.11.2 typing-extensions-4.8.0 wcwidth-0.2.8 zipp-3.17.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0msed: can't read /usr/local/lib/python3.9/dist-packages/google/colab/*.py: No such file or directory\n",
            "sed: can't read /usr/local/lib/python3.9/dist-packages/google/colab/*.py: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip3 install torch torchvision\n",
        "!pip install biopython\n",
        "!pip install biopython==1.76\n",
        "!pip install attrdict\n",
        "!pip install matplotlib\n",
        "!pip install sklearn\n",
        "!pip install scikit-learn\n",
        "!pip install forgi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M9kHk7sQ0S4S",
        "outputId": "7ed5fd8b-b1be-477e-c436-1e69bc05c6e0"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/75/cd/7ae0f2cd3fc68aea6cfb2b7e523842e1fa953adb38efabc110d27ba6e423/numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m51.2/58.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.26.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch\n",
            "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d3/fa/93f3ef65dee8947d8f54eb876ab57a8b019845a45dc07546e2ac214da97b/torch-2.1.0-cp39-cp39-manylinux1_x86_64.whl.metadata\n",
            "  Downloading torch-2.1.0-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision\n",
            "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/8b/64/00e53316beb2f19edcaa0eb283b6087cb4d5275112f24b8ea0d8b49c1d5f/torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl.metadata\n",
            "  Downloading torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
            "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.8.0)\n",
            "Collecting sympy (from torch)\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx (from torch)\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2 (from torch)\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec (from torch)\n",
            "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
            "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch)\n",
            "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/d1/5a/e5811fcc8fc6703be39eb157af6224eaa3b628a42008df93b87e23eb9731/triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
            "  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/0a/f8/5193b57555cbeecfdb6ade643df0d4218cc6385485492b6e2f64ceae53bb/nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata\n",
            "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.26.0)\n",
            "Collecting requests (from torchvision)\n",
            "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://files.pythonhosted.org/packages/0a/20/a94a0462495de73e248643fb24667270f2e67f44792456ab7207764e80cc/Pillow-10.0.1-cp39-cp39-manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading Pillow-10.0.1-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/de/63/cb7e71984e9159ec5f45b5e81e896c8bdd0e45fe3fc6ce02ab497f0d790e/MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->torchvision)\n",
            "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/9e/45/824835a9c165eae015eb7b4a875a581918b9fc96439f8d9a5ca0868f0b7d/charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchvision)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->torchvision)\n",
            "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/26/40/9957270221b6d3e9a3b92fdfba80dd5c9661ff45a664b47edd5d00f707f5/urllib3-2.0.6-py3-none-any.whl.metadata\n",
            "  Downloading urllib3-2.0.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchvision)\n",
            "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting mpmath>=0.19 (from sympy->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.0-cp39-cp39-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-10.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
            "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading urllib3-2.0.6-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, urllib3, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
            "Successfully installed MarkupSafe-2.1.3 certifi-2023.7.22 charset-normalizer-3.3.0 filelock-3.12.4 fsspec-2023.9.2 idna-3.4 jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.2.140 nvidia-nvtx-cu12-12.1.105 pillow-10.0.1 requests-2.31.0 sympy-1.12 torch-2.1.0 torchvision-0.16.0 triton-2.1.0 urllib3-2.0.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/3.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/3.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython) (1.26.0)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting biopython==1.76\n",
            "  Using cached biopython-1.76.tar.gz (16.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython==1.76) (1.26.0)\n",
            "Building wheels for collected packages: biopython\n",
            "  Building wheel for biopython (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for biopython: filename=biopython-1.76-cp39-cp39-linux_x86_64.whl size=2603383 sha256=767b3cd713e0ec28052782d48b9805e13f206669b5aeb1ea71190e714e35c5c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/1c/09/f3cca391e94687de244194019727d61885b4c252bb2d8b1588\n",
            "Successfully built biopython\n",
            "Installing collected packages: biopython\n",
            "  Attempting uninstall: biopython\n",
            "    Found existing installation: biopython 1.81\n",
            "    Uninstalling biopython-1.81:\n",
            "      Successfully uninstalled biopython-1.81\n",
            "Successfully installed biopython-1.76\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "Bio"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting attrdict\n",
            "  Using cached attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from attrdict) (1.16.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting matplotlib\n",
            "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/e0/8b/b62bc50b01bb2d4af96bc0045c39d60209e2701e172789ceace20a0866b2/matplotlib-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading matplotlib-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/2b/c0/24c34c41a180f875419b536125799c61e2330b997d77a5a818a3bc3e08cd/contourpy-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading contourpy-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/06/19/05e6d60206300d030948c2e09c28ad7f6e3c6e2299b9a5beb01261b38f0a/fonttools-4.43.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading fonttools-4.43.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
            "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/c0/a8/841594f11d0b88d8aeb26991bc4dac38baa909dc58d0c4262a4f7893bcbf/kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
            "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (10.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
            "  Obtaining dependency information for importlib-resources>=3.2.0 from https://files.pythonhosted.org/packages/65/6e/09d8816b5cb7a4006ef8ad1717a2703ad9f331dae9717d9f22488a2d6469/importlib_resources-6.1.0-py3-none-any.whl.metadata\n",
            "  Downloading importlib_resources-6.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading matplotlib-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.9/301.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.43.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
            "Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.43.1 importlib-resources-6.1.0 kiwisolver-1.4.5 matplotlib-3.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "kiwisolver"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post9.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting scikit-learn\n",
            "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/af/ad/329a88013936e4372181c0e275c19aa6130b0835876726944b811af5a856/scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.26.0)\n",
            "Collecting scipy>=1.5.0 (from scikit-learn)\n",
            "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/88/8c/9d1f74196c296046af1f20e6d3fc7fbb27387282315e1643f450bba14329/scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
            "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.3.1 scipy-1.11.3 threadpoolctl-3.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting forgi\n",
            "  Obtaining dependency information for forgi from https://files.pythonhosted.org/packages/4d/87/785b11d57bf235cf35de5eae23b455a5281de184d08265a256a0c375bf39/forgi-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading forgi-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from forgi) (1.26.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from forgi) (1.11.3)\n",
            "Collecting pandas>=0.20 (from forgi)\n",
            "  Obtaining dependency information for pandas>=0.20 from https://files.pythonhosted.org/packages/bc/7e/a9e11bd272e3135108892b6230a115568f477864276181eada3a35d03237/pandas-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading pandas-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting future (from forgi)\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.9/dist-packages (from forgi) (3.1)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.9/dist-packages (from forgi) (1.76)\n",
            "Collecting appdirs>=1.4 (from forgi)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting logging-exceptions>=0.1.9 (from forgi)\n",
            "  Using cached logging_exceptions-0.1.9.tar.gz (7.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cython (from forgi)\n",
            "  Obtaining dependency information for cython from https://files.pythonhosted.org/packages/51/81/a1b6c0fdc987b37ab3fc7c683b396b55746e3067e71d8a9435e1d2f3b2d6/Cython-3.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading Cython-3.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.20->forgi) (2.8.2)\n",
            "Collecting pytz>=2020.1 (from pandas>=0.20->forgi)\n",
            "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
            "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas>=0.20->forgi)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20->forgi) (1.16.0)\n",
            "Downloading forgi-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Cython-3.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: logging-exceptions, future\n",
            "  Building wheel for logging-exceptions (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logging-exceptions: filename=logging_exceptions-0.1.9-py3-none-any.whl size=7942 sha256=efb25e21980cc11d81b7f6e8d5951cab76c073638f086bd7e3c56788e709cb9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/da/a4/fa831f34f9581286177e1235166329ab74ffa395f8e247ab68\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=53fc24aec706b0f2d0edda3b4bc0a5fefa630b3fed3864add55bb54ae08e8900\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
            "Successfully built logging-exceptions future\n",
            "Installing collected packages: pytz, logging-exceptions, appdirs, tzdata, future, cython, pandas, forgi\n",
            "Successfully installed appdirs-1.4.4 cython-3.0.3 forgi-2.2.2 future-0.18.3 logging-exceptions-0.1.9 pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytz"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd RNABERT && wget https://raw.githubusercontent.com/AnzorGozalishvili/NASA_ODSR_DATA/main/bert_mul_2.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4x-rpSW1AQn",
        "outputId": "84e96285-f393-4536-b8bd-2666aab92727"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-08 10:28:21--  https://raw.githubusercontent.com/AnzorGozalishvili/NASA_ODSR_DATA/main/bert_mul_2.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2084567 (2.0M) [application/octet-stream]\n",
            "Saving to: ‘bert_mul_2.pth’\n",
            "\n",
            "\rbert_mul_2.pth        0%[                    ]       0  --.-KB/s               \rbert_mul_2.pth      100%[===================>]   1.99M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-10-08 10:28:21 (25.2 MB/s) - ‘bert_mul_2.pth’ saved [2084567/2084567]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd RNABERT && \\\n",
        "python3 MLM_SFP.py \\\n",
        "    --pretraining bert_mul_2.pth \\\n",
        "    --data_embedding  sample/mlm/RF00001.fa \\\n",
        "    --embedding_output ./predictions.txt \\\n",
        "    --batch 40"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRkmZ6DZ1FMJ",
        "outputId": "8a1947ae-14f3-4d68-9d3d-9ed8423893d3"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/RNABERT/MLM_SFP.py\", line 3, in <module>\n",
            "    import numpy as np\n",
            "ModuleNotFoundError: No module named 'numpy'\n"
          ]
        }
      ]
    }
  ]
}